{"decks": [{"name": "Roam2Anki", "cards": [{"type": "cloze", "id": "6270b9ca-40fe-489b-9362-cb1a1425c6bf ", "text": " What is an eigenvector? {{c1:: Eigenvectors is a decomposition of a matrix, where we multiply the matrix by a vector, and the result is the same vector multiplied by an scalar \\(A*\\vec{v} = \\lambda * \\vec{v}\\)  }} {{id: 6270b9ca-40fe-489b-9362-cb1a1425c6bf }} [[FlashCard]]"}, {"type": "cloze", "id": "1374e440-d316-4fb9-ac95-ba7aa10cbbc4 ", "text": " What is the geometric explanation of eigenvectors? {{c1:: it will tell you, which directions while not change when using the transformation matrix `A`, the eigenvectors will not change direction, they will only stretch by the factor of the eigenvalue (\\( \\lambda \\))   }} {{id: 1374e440-d316-4fb9-ac95-ba7aa10cbbc4 }} [[FlashCard]]"}, {"type": "cloze", "id": "770e5a7b-753c-4197-8e78-0b95c66f4304 ", "text": "How does GraalVM achieve more performance than the JVM {{c1:: removing the linker phase in runtime (JAVA_PATH) and jumping the step of bytecode. This wayit can apply good optimization }} at the expense of {{c2: Loosing some reflection capabilities and the compile once and run everywhere}} {{id: 770e5a7b-753c-4197-8e78-0b95c66f4304 }} [[FlashCard]]"}, {"type": "cloze", "id": "2a50c915-ab11-4584-96aa-fcd9955b3107 ", "text": " What is Truffle {{c1:: is an abstract syntax tree interpreter that allow to build frontend for GraalVM  }} {{id: 2a50c915-ab11-4584-96aa-fcd9955b3107 }} [[FlashCard]]"}, {"type": "cloze", "id": "c1162ebd-c46f-4b02-a1fa-218bba326287 ", "text": "What is Keycloack? {{c1:: is a Open Source Identity and Access Management,  }} {{id: c1162ebd-c46f-4b02-a1fa-218bba326287 }} [[FlashCard]]"}, {"type": "cloze", "id": "1a6bef7c-f98c-4367-86c6-06a4d83a2f40 ", "text": " Similar services of keycloack? {{c1:: Okta and Auth0}} {{id: 1a6bef7c-f98c-4367-86c6-06a4d83a2f40 }} [[FlashCard]]"}, {"type": "cloze", "id": "00094e21-2a39-459c-ba66-4dfdaa60eee9", "text": "* One easy way to convert a text to an array of numbers is by using a technique named {{c1:: Bag of Words}} which consist of {{c2:: converting every word in the vocabulary into an index, and then a sentence is 0 or 1 depending if the word exist }} {{id: 00094e21-2a39-459c-ba66-4dfdaa60eee9}} [[FlashCard]] "}, {"type": "cloze", "id": "42c5aae1-b1e1-4321-84f8-920c9d80175f ", "text": "* With an spare representation a logistic regression model will need to learn {{c1:: n+1}} parameters where n is the size of the vocabulary {{id: 42c5aae1-b1e1-4321-84f8-920c9d80175f }} [[FlashCard]]"}, {"type": "cloze", "id": "32681a27-db31-45e7-9853-148cb304f8a1 ", "text": "* With a frequency table we can create a frequency representation of the tweet as {{c1:: [1 <- bias, sum of positive frequencies, sum of negative frequencies] }} {{id: 32681a27-db31-45e7-9853-148cb304f8a1 }} [[FlashCard]]"}, {"type": "cloze", "id": "b45287f1-2711-4bae-82f8-7afb8f9a7eff ", "text": "-  What P(A | B ) means?  {{c1:: frequency of happening A if we know that B already happened  }} {{id: b45287f1-2711-4bae-82f8-7afb8f9a7eff }} [[FlashCard]]"}, {"type": "cloze", "id": "ebcfb3d7-e93e-486b-a368-a99054df14d1 ", "text": "-   How do you express the probability of A, once we already now event B happened {{c1:: P(A | B)  }} {{id: ebcfb3d7-e93e-486b-a368-a99054df14d1 }} [[FlashCard]]"}, {"type": "cloze", "id": "531ecec2-509d-4d73-9799-a774d4f52a8a ", "text": "\t-  how can you rewrite \\(P(A | B)\\) using Bayes Rule? {{c1::  \\(P(A | B) = P(B | A) * \\frac {P(A)} {P(B)}\\) }} {{id: 531ecec2-509d-4d73-9799-a774d4f52a8a }} [[FlashCard]]"}, {"type": "cloze", "id": "abcfad02-0b85-4fa0-8b3c-ee9fd01cfa4e ", "text": "-  What does \\(P(Positive | \"happy\")\\)  mean? {{c1:: It means, probability of being of class \"positive\" given that it contains the word \"happy\"  }} {{id: abcfad02-0b85-4fa0-8b3c-ee9fd01cfa4e }} [[FlashCard]]"}, {"type": "cloze", "id": "98b50522-a207-4b98-ac16-a16d3c3ca76e ", "text": " - How you calculate the vector norm (euclidean distance) in numpy? {{c1:: `np.linalg.norm(a - b)` }} {{id: 98b50522-a207-4b98-ac16-a16d3c3ca76e }} [[FlashCard]]"}, {"type": "cloze", "id": "7a0af8e4-9f9d-4477-8f89-3e104b5f2e9b ", "text": "- What is the problem of using euclidean distance for word vector representation? {{c1::  depending on number of occurrence of the words, the distance can be different }} and what we can use to overcome this problem? {{c2:: Cosine similarity}} {{id: 7a0af8e4-9f9d-4477-8f89-3e104b5f2e9b }} [[FlashCard]]"}, {"type": "cloze", "id": "CostaRica [[July 22nd, 2020]]", "text": "- Capital of Costa rica is {{c1:: San Jose}} {{id: CostaRica [[July 22nd, 2020]]}}[[FlashCard]] "}, {"type": "cloze", "id": "Papua[[July 22nd, 2020]]", "text": "- Capital of Papua New Guinea is {{c1:: Port Moresby}} {{id: Papua[[July 22nd, 2020]]}} [[FlashCard]] "}, {"type": "cloze", "id": "31d677e6-760b-481a-ae6d-f3b5ef5ca189 ", "text": "How do you calculate the dot product of two vectors? {{c1:: \\( \\vec{v}*\\vec{w} = \\sum {v_i * w_i} \\)}} {{id: 31d677e6-760b-481a-ae6d-f3b5ef5ca189 }} [[FlashCard]]"}, {"type": "cloze", "id": "8783b050-b91d-4f7d-a1b7-48563a7b0695 ", "text": "-  Why is Naive Bayes, named naive? {{c1:: Cause it makes the assumption that features used for classification are independent }} {{id: 8783b050-b91d-4f7d-a1b7-48563a7b0695 }} [[FlashCard]]"}, {"type": "cloze", "id": "70f6e5af-a4af-448d-9849-a35707d7df74 ", "text": "\t-  Why we use Log Likelihood {{c1:: for numeric stability, preventing underflow }} {{id: 70f6e5af-a4af-448d-9849-a35707d7df74 }} [[FlashCard]]"}, {"type": "cloze", "id": "b3a764f7-9a8d-422a-bad9-4da27f66a3be ", "text": "\t-  how you can decompose \\(log(a * b)\\) {{c1:: \\(log(a) + log(b)\\)}} {{id: b3a764f7-9a8d-422a-bad9-4da27f66a3be }} [[FlashCard]]"}, {"type": "cloze", "id": "9691cc2a-14f8-4d4d-912a-a953da1fa294 ", "text": "Logistic regression cost function is  {{c1:: -1/m of Sum of  }}  {{c2:: `y(i) * log h(x(i), theta)` }}  {{c3:: `1-y(i) * log(1-h(x(i), theta))` }} {{id: 9691cc2a-14f8-4d4d-912a-a953da1fa294 }} [[FlashCard]]"}, {"type": "cloze", "id": "5ef0e3bf-f6e7-4177-a929-7605097aa597 ", "text": " Stemming is the process of  {{c1:: reducing words to they root meaning }} {{id: 5ef0e3bf-f6e7-4177-a929-7605097aa597 }} [[FlashCard]]"}, {"type": "cloze", "id": "73d40fb1-23b0-48cf-be15-f7c52b582bd7 ", "text": " We do stemming cause {{c1:: reduces the vocabulary size maintaining the meaning of the word}} {{id: 73d40fb1-23b0-48cf-be15-f7c52b582bd7 }} [[FlashCard]]"}, {"type": "cloze", "id": "d969cb5a-dcf3-461d-ae99-b3bcc0ae15b8 ", "text": "What is  a data scientist? {{c1::somebody who is a better developer than most statisticians, and somebody who is a better statisticians than most developers  }} {{id: d969cb5a-dcf3-461d-ae99-b3bcc0ae15b8 }} [[FlashCard]]"}, {"type": "cloze", "id": "b8631cac-e4ab-41ab-8848-24715486edce ", "text": " What is theia IDE? {{c1::  is an online editor, for remote and desktop, quite similar to vscode}} {{id: b8631cac-e4ab-41ab-8848-24715486edce }} [[FlashCard]]"}, {"type": "cloze", "id": "bc14105b-9b12-4c67-b196-31b65df07893 ", "text": "Why validation set is used to know if we are over fitting?  {{c1:: the idea is that you your function generalize to images/data that is has not seen before. that's exactly the function of the validation set, we can now we are overfitting if the training loss is way lower than the validation loss, that would mean  }} {{id: bc14105b-9b12-4c67-b196-31b65df07893 }} [[FlashCard]]"}, {"type": "cloze", "id": "2de336de-a616-4c48-b22b-31976a0906dc ", "text": "Why the loss function cannot be the accuracy or the error rate?  {{c1:: cause error rate and accuracy is not derivable, which means that if we modify the model parameters is possible we don't see difference in the error/accuracy  }} {{id: 2de336de-a616-4c48-b22b-31976a0906dc }} [[FlashCard]]"}, {"type": "cloze", "id": "f0c8a9c1-7f4e-4a96-888a-390f747b1525 ", "text": "How to create validation set for forecast?   {{c1:: training set can be all the data except last 2 weeks, validation/test set the rest. Why? cause if you hvae future data in the model it's just cheating  }} {{id: f0c8a9c1-7f4e-4a96-888a-390f747b1525 }} [[FlashCard]]"}, {"type": "cloze", "id": "34772750-8ba7-4b08-9995-ca1341ee42ba ", "text": "How you can overfit to validation set? {{c1::  by turning the hyper parameters, as we do different tries. at the end it's possible we adjust the best parameters to the validation set. That's why should havea test set }} {{id: 34772750-8ba7-4b08-9995-ca1341ee42ba }} [[FlashCard]]"}, {"type": "cloze", "id": "07e36233-a38d-420d-a91e-45a9fc40ef53 ", "text": " What is Catastrophic forgetting?  {{c1::   }} {{id: 07e36233-a38d-420d-a91e-45a9fc40ef53 }} [[FlashCard]]"}, {"type": "cloze", "id": "68f5a9f6-8f67-41c9-8985-9c17b7a58650 ", "text": "assuming tensor is a rank2 tensor, What does tensor[:, 1] does? {{c1:: Give me a tensor of rank 1, which is the first colum of the matrix}} {{id: 68f5a9f6-8f67-41c9-8985-9c17b7a58650 }} [[FlashCard]]"}, {"type": "cloze", "id": "5c847730-245a-h415c-9e01-c1f4e7c54ada ", "text": "How can you rewrite tensor[1,:]? {{c1::  tensor[1] }} {{id: 5c847730-245a-h415c-9e01-c1f4e7c54ada }} [[FlashCard]]"}, {"type": "cloze", "id": "bb61083b-80c1-49be-a68d-5d9e3cdfdf8c ", "text": "assuming tensor is a rank2 tensor, what is the x and y of tensor[x ,y]?  {{c1:: x selects the first axis,which is the rows, and y is the second axis which is the columns  }} {{id: bb61083b-80c1-49be-a68d-5d9e3cdfdf8c }} [[FlashCard]]"}, {"type": "cloze", "id": "27a0601d-3571-4522-a524-bdedab0cb295 ", "text": "If I have a list of 1000 elements of tensors 2 rank, how can I create a tensor of rank 3, as the shape is (1000, ...,... ) which  {{c1:: torch.stack(list_of_tensor)  }} {{id: 27a0601d-3571-4522-a524-bdedab0cb295 }} [[FlashCard]]"}, {"type": "cloze", "id": "0bbf45df-8310-4b4e-8460-3ff5ba5f2f48 ", "text": " Given a tensor of shape 1000, 28, 28. how will you get the mean tensor of size 1000? or in other words the Mean of the 28 for 28 values for each 1000 items {{c1::  tensor.mean(-1, -2) -> make the mean around the last and the second-last axis.}} {{id: 0bbf45df-8310-4b4e-8460-3ff5ba5f2f48 }} [[FlashCard]]"}, {"type": "cloze", "id": "0f16bfe9-3194-496b-b2cf-e30cfc2e9bb1 ", "text": "Given a tensor t,what is the difference between `t.requires_grad()` and `t.requires_grad_()` {{c1:: `_` at the end of a function in pytorch means that will modify modify the value (mutate it), but normally pytorch methods are inmutable meaning that will return a new tensor  }} {{id: 0f16bfe9-3194-496b-b2cf-e30cfc2e9bb1 }} [[FlashCard]]"}, {"type": "cloze", "id": "89a06c4-5a06-44fe-a572-ec72d1dd2eb5 ", "text": " how you decide to stop training?  {{c1:: looking at training and validation losses or when we run out of time computationally  }} {{id: 89a06c4-5a06-44fe-a572-ec72d1dd2eb5 }} [[FlashCard]]"}, {"type": "cloze", "id": "ae9a1525-a703-4278-ad8e-c1aaccd23bcd ", "text": "if you had a tensor of shape [1000] and you want to convert to shape [1000,1] howcan you do it? {{c1:: t.unsqueeze(1) }} {{id: ae9a1525-a703-4278-ad8e-c1aaccd23bcd }} [[FlashCard]]"}, {"type": "cloze", "id": "73e65d4b-ccf6-479d-9b54-e56e5d3aa193 ", "text": "What is the compromise between all-batch or single image batch? {{c1:: quality of the gradient vs number of times we update the gradient. If we use the whole dataset to update the gradients. The gradients are going to be really stable and have information of all the dataset, but we are going to update only once, making the training slower. If we use all the dataset, we are going to update the weights a lot of times, but since the quality of the gradient is poor, it will take a lot of time to converge}} {{id: 73e65d4b-ccf6-479d-9b54-e56e5d3aa193 }} [[FlashCard]]"}, {"type": "cloze", "id": "bc43222f-bace-4614-956e-614381cc7bb2 ", "text": "What is the relationship between learning rate and batch-size? {{c1:: If the batch-size is bigger, means that the gradients are more stable, that means we can do bigger jumps. with adam optimizers etc... we have not to go crazy, but if we change the batch-size substantially, we should also change the lr for the sqrt() }} {{id: bc43222f-bace-4614-956e-614381cc7bb2 }} [[FlashCard]]"}, {"type": "cloze", "id": "a6a25155-5932-44a4-912c-dc57399d091a ", "text": "Why we need a relu or any other non-linearity to create a NN? {{c1:: You need the non linearity to add new lyaers to the NN, if not the combination of two linear operationn (matrix multiplication) can be combined to a single multiplication }} {{id: a6a25155-5932-44a4-912c-dc57399d091a }} [[FlashCard]]"}, {"type": "cloze", "id": "1f9dfacf-3f15-4f44-a2fe-e17c7b921df2 ", "text": "What does the Universal approximation theory says? {{c1:: that a NN with 2 layers, with enough parameters will be able to approximate any function.  }} {{id: 1f9dfacf-3f15-4f44-a2fe-e17c7b921df2 }} [[FlashCard]]"}, {"type": "cloze", "id": "0ecf7618-252c-4b95-aff5-d669ee1e1a08 ", "text": "Softmax formula? {{c1:: \\(softmax(x, class) = \\frac{exp(x_{class})} {\\sum_{i=x}exp(x_i)}\\)  }} {{id: 0ecf7618-252c-4b95-aff5-d669ee1e1a08 }} [[FlashCard]]"}, {"type": "cloze", "id": "78158fdd-c8d3-4597-8c7d-bf8afbe44a19 ", "text": " What is the log_softmax function in pytorch? {{c1:: Calculates the log of the softmax, which is easier do while doing the softmax than latter  }} {{id: 78158fdd-c8d3-4597-8c7d-bf8afbe44a19 }} [[FlashCard]]"}, {"type": "cloze", "id": "b75aaa6f-8092-462e-a858-455dd876432a ", "text": "When to use CrossEntropyLoss? {{c1:: For classification problems, that have more than one category  }} {{id: b75aaa6f-8092-462e-a858-455dd876432a }} [[FlashCard]]"}, {"type": "cloze", "id": "cece6bbf-d968-4539-b7a4-4e9481bec4b5 ", "text": "What does `nn.CrossEntropyLoss(reduce=none)` does? {{c1::  give you the loss for each input, by default the reduce mode is mean, and give you a single value (the mean of all of them) }} {{id: cece6bbf-d968-4539-b7a4-4e9481bec4b5 }} [[FlashCard]]"}, {"type": "cloze", "id": "045ce9db-b3df-4dfa-84c1-340e483784f2 ", "text": "How you define CrossEntropyLoss?   {{c1::  \\(loss(x, class) = -log * softmax(x, class)\\) which is equal to \\(-x[class] + log(\\sum_{j} exp(x_j))\\) }} {{id: 045ce9db-b3df-4dfa-84c1-340e483784f2 }} [[FlashCard]]"}, {"type": "cloze", "id": "19f8dee5-7b8c-4922-8ab8-a2a94dbc58cd ", "text": "How does learning rate finder works? {{c1:: we start training the model with a very small lr, and we increase it on each epoch, and we plot the loss and the lr. At some point loss is going to go up, since it will be too big. We want to use the lr that got the error down faster. A good rule of thumb is to get the 1/10 of the bottom of the chart.}} {{id: 19f8dee5-7b8c-4922-8ab8-a2a94dbc58cd }} [[FlashCard]]"}, {"type": "cloze", "id": "d6932871-8a06-40d6-9c4f-b8cac5a9a303 ", "text": "}} {{id: d6932871-8a06-40d6-9c4f-b8cac5a9a303 }} [[FlashCard]]"}, {"type": "cloze", "id": "24f0e898-005a-4ae6-a4a5-d366938023df ", "text": "What is discriminative learning rate?  {{c1:: When doing fine-tunning, we know that the ending layers might be more specialized to ImageNet, so they need to change more than the earlier ones, so the idea is to apply a different learning rate to each layer, in the course they use a min and max learning rate, for the first and last layers, and interpolate the lr for the intermediate layers  }} {{id: 24f0e898-005a-4ae6-a4a5-d366938023df }} [[FlashCard]]"}, {"type": "cloze", "id": "4e8c98d9-543d-41ab-a8fb-c6cedbda10f2 ", "text": "how to get all the values of the 1st column of all rows with pandas? {{c1:: `iloc[:, 0]`  }} {{id: 4e8c98d9-543d-41ab-a8fb-c6cedbda10f2 }} [[FlashCard]]"}, {"type": "cloze", "id": "eda3fc65-ac6b-4a14-94f1-5b6019ab830d ", "text": " how to get the first row in pandas {{c1:: `df[0,:]` or `df[0]`  }} {{id: eda3fc65-ac6b-4a14-94f1-5b6019ab830d }} [[FlashCard]]"}, {"type": "cloze", "id": "d296da26-693d-4ea6-8084-8880eba01771 ", "text": " What is a regularization technique?  {{c1:: they are techniques that penalize the model when it tries to overfit, allowing training for more time and get better results, if we overfit to fast  }} {{id: d296da26-693d-4ea6-8084-8880eba01771 }} [[FlashCard]]"}, {"type": "cloze", "id": "12614845-25d8-4d2f-8190-1f5674e20bb4 ", "text": " What is weight decay? {{c1:: a regularization technique, also known as L2 regularization. It connsists on adding a new term to the loss function. The sum of all the weights square. Now the SGD algorithm can minimize the loss by finding weights that are small. And smaller makes a loss surface which is less complex. \\(loss = loss + wd * \\sum_{} weights^2\\)    }} {{id: 12614845-25d8-4d2f-8190-1f5674e20bb4 }} [[FlashCard]]"}, {"type": "cloze", "id": "2160edc0-1e69-4567-acbf-f15b88594783 ", "text": "For trees, what should we do with dates columns? {{c1:: do some feature engineer and create columns from it, we can use fast ai `add_datepart`  }} {{id: 2160edc0-1e69-4567-acbf-f15b88594783 }} [[FlashCard]]"}, {"type": "cloze", "id": "55eab98f-ace6-4e8e-8b37-caf07e7aa539 ", "text": "}} {{id: 55eab98f-ace6-4e8e-8b37-caf07e7aa539 }} [[FlashCard]]"}, {"type": "cloze", "id": "a9eea9f5-f08b-4e77-9c4b-1e7417bc3ec4 ", "text": "How to stop Decision trees from overfit without ensemble? {{c1:: specify the min number of data points that a leaf node should have, this way the model can not split further until having a leaf node for each data point  }} {{id: a9eea9f5-f08b-4e77-9c4b-1e7417bc3ec4 }} [[FlashCard]]"}, {"type": "cloze", "id": "2002658f-3dda-441a-81b0-ef0b2c31ce07 ", "text": "}} {{id: 2002658f-3dda-441a-81b0-ef0b2c31ce07 }} [[FlashCard]]"}, {"type": "cloze", "id": "1997d8d8-2190-4ace-9526-5772cbbadea0 ", "text": "What is a random forest? {{c1:: is similar to how bagging works, but not only does random rows for bootstrapping, but also does random columns. Create multiple trees, and get the average of the predictions in inference time  }} {{id: 1997d8d8-2190-4ace-9526-5772cbbadea0 }} [[FlashCard]]"}, {"type": "cloze", "id": "ed4526d7-0c25-457e-8f6f-e881a6537819 ", "text": "What is OOB in random forest (Out-Of-Bag) error? {{c1:: instead of splitting the data in train/validation, for each datapoint we get the error of the trees that didn't contain the row. the idea is quite similar to CV. this way you can use all the data }} {{id: ed4526d7-0c25-457e-8f6f-e881a6537819 }} [[FlashCard]]"}, {"type": "cloze", "id": "9c6c3742-20a6-467e-b691-bac357971c3e ", "text": "What is one downside of trees, about predicting? {{c1:: trees basically cannot extrapolate the function, so basically they can give just results they already saw, if we had a trend upwards like sales is increasing year after year, and we fit a tree for all the years except the last one, and we try to infer. The error will be massive, cause the tree can only give values it saw in the training set, it cannot extrapolate the trend  }} {{id: 9c6c3742-20a6-467e-b691-bac357971c3e }} [[FlashCard]]"}, {"type": "cloze", "id": "8212c707-3148-45a1-887e-655969a0e717 ", "text": "}} {{id: 8212c707-3148-45a1-887e-655969a0e717 }} [[FlashCard]]"}, {"type": "cloze", "id": "d34fd830-2a54-4d63-81ac-b1318b6eb83f ", "text": "What is a language model? {{c1:: a model that predicts the next word of a document. }} {{id: d34fd830-2a54-4d63-81ac-b1318b6eb83f }} [[FlashCard]]"}, {"type": "cloze", "id": "ffba23bd-426f-43e7-b572-ea3dfdcb23fe ", "text": "-What is cosine similarity? {{c1::  is a metric to compare two vectors depending on the inner angle, and that way, it's not affected for the size of the vector}} {{id: ffba23bd-426f-43e7-b572-ea3dfdcb23fe }} [[FlashCard]]"}, {"type": "cloze", "id": "c19d6406-76f5-4088-8afe-6771b6d8f78d ", "text": "-   Cosine of two vectors is: {{c1::  \\(cos(Beta) = \\frac {\\vec(v) * \\vec(w)} {\\|\\vec(v)\\| * \\|\\vec{w}\\|} \\) }} {{id: c19d6406-76f5-4088-8afe-6771b6d8f78d }} [[FlashCard]]"}]}]}